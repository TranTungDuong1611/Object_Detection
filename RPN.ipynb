{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "cell_execution_strategy": "setup",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Import các thư viện cần thiết**"
      ],
      "metadata": {
        "id": "NEeo6kJ9YELo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrPugiR3qZ4j"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tải mô hình VGG16 đã được huấn luyện trước**:\n",
        "=> Để trích xuất feature maps làm đầu vào cho RPN. Ở đây chỉ lấy phần đặc trưng (features), bỏ qua các lớp fully connected.\n",
        "\n",
        "## Một số thông tin về trích xuất đặc trưng bằng VGG 16\n",
        "\n",
        "- VGG16 có kiến trúc bao gồm nhiều lớp tích chập và pooling giúp nó tạo ra các bản đồ đặc trưng (feature maps) từ ảnh. Các lớp này có khả năng trích xuất đặc trưng theo nhiều cấp độ khác nhau, từ các đặc trưng đơn giản (ví dụ như cạnh viền) đến các đặc trưng phức tạp (ví dụ như khuôn mặt, đồ vật).\n"
      ],
      "metadata": {
        "id": "s7x7Ckq1UiGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VGG16_model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)"
      ],
      "metadata": {
        "id": "BODv1fEMg0-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Xây dựng mạng Backbone**\n",
        "\n",
        "Backbone sẽ nhận đầu vào là ảnh và tạo ra feature map (bản đồ đặc trưng) làm đầu vào cho các lớp tiếp theo.\n",
        "Ở đây sẽ chỉ lấy các feature và bỏ qua fully connected layers (dùng cho classification)\n",
        "\n"
      ],
      "metadata": {
        "id": "uLEZmcwwcY_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_back_bone = nn.Sequential(*list(VGG16_model.features.children())[:-1])\n",
        "#list(VGG16_model.features.children(): chuyển các lớp con thành một danh sách\n",
        "#[:-1]: bỏ lớp cuối (lớp pooling)"
      ],
      "metadata": {
        "id": "ql6O3gtUk-Bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anchor_ratio = [[1, 1], [1, 2], [2, 1]] #tỉ lệ các anchor\n",
        "anchor_size = [128, 256, 512]\n",
        "\n",
        "class RPN(nn.Module):\n",
        "    def __init__(self,input_channels, k=9):\n",
        "        #Convolution Layers\n",
        "        super(RPN,self).__init__()\n",
        "        self.conv = nn.Conv2d(input_channels, 512, 3, 1, 1) # 512 filters, kernel 3X3\n",
        "        self.cls = nn.Conv2d(512, k, 1) #=0 -> k chứa vật thể / =1 -> chứa vật thể\n",
        "        self.reg = nn.Conv2d(512, 4 * k, 1) #\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "            output: cls:[B, 9, H, W], reg:[B, 36, H, W]\n",
        "        '''\n",
        "        x = F.relu(self.conv(x))\n",
        "        cls = F.sigmoid(self.cls(x))\n",
        "        reg = self.reg(x)\n",
        "        return cls, reg\n"
      ],
      "metadata": {
        "id": "fd_aLUkZqp9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RPNLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RPNLoss,self).__init__()\n",
        "        self.cls_loss = nn.BCELoss()\n",
        "        self.reg_loss = nn.SmoothL1Loss(reduction='mean')\n",
        "        self.lam_reg = 10\n",
        "        self.k = 9\n",
        "\n",
        "    def forward(self, cls_pred, cls_gt, reg_pred, reg_gt,):\n",
        "        '''\n",
        "            cls_shape = [B, T, 1] T = 256\n",
        "            reg_shape = [B, N, M, 4] N*M = 2400\n",
        "        '''\n",
        "        N_cls = cls_gt.shape[0]\n",
        "\n",
        "        cls_loss = self.cls_loss(cls_pred, cls_gt)\n",
        "\n",
        "\n",
        "        reg_pred = reg_pred.permute(0, 2, 3, 1)\n",
        "        size_batch_reg = reg_pred.shape[0]\n",
        "        reg_pred = reg_pred.reshape(size_batch_reg, reg_pred.shape[1]* reg_pred.shape[2], 4*self.k)\n",
        "\n",
        "        reg_gt = reg_gt.permute(0, 2, 3, 1)\n",
        "        reg_gt = reg_gt.reshape(size_batch_reg, reg_gt.shape[1]* reg_gt.shape[2], 4*self.k)\n",
        "        pos_gt = reg_gt > 0\n",
        "        # pos_count = torch.zeros_like(reg_gt)\n",
        "        # pos_count[pos_gt] = 1\n",
        "        reg_loss = torch.sum(self.reg_loss(reg_pred[pos_gt], reg_gt[pos_gt]))\n",
        "        loss = cls_loss + self.lam_reg * reg_loss * 4\n",
        "        return loss"
      ],
      "metadata": {
        "id": "hpxKFmwVwlh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = RPNLoss()"
      ],
      "metadata": {
        "id": "SKgF9rDX34rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d zaraks/pascal-voc-2007"
      ],
      "metadata": {
        "id": "akH_4XSij4Ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip pascal-voc-2007.zip -d pascal-voc-2007"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ggPuwad3vlRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size_img = [600, 600]"
      ],
      "metadata": {
        "id": "0498CEMV4ZTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_coord(coord, scale=16):\n",
        "    return coord*scale\n",
        "\n",
        "def get_feature_size(size, scale=16):\n",
        "    return [size[-2]//scale, size[-1]//scale]\n",
        "\n",
        "def convert_box(bbox, btype='conner'):\n",
        "    bboxes = bbox.clone()\n",
        "    if (btype == 'conner'):\n",
        "        bboxes[...,:2] -= bboxes[..., 2:]/2\n",
        "        bboxes[...,2:] += bboxes[..., :2]\n",
        "        return bboxes\n",
        "\n",
        "    elif (btype == 'center'):\n",
        "        bboxes[:, :2] += bboxes[:, 2:]\n",
        "        bboxes[:, :2] /= 2\n",
        "        bboxes[:, 2:] -= bboxes[:, :2]\n",
        "        bboxes[:, 2:] *= 2\n",
        "        return bboxes\n",
        "\n",
        "def IoU(box1, box2):\n",
        "    box1 = convert_box(box1)\n",
        "    box2 = convert_box(box2)\n",
        "    U_x_min = torch.min(box1[...,0], box2[...,0])\n",
        "    U_y_min = torch.min(box1[...,1], box2[...,1])\n",
        "    U_x_max = torch.max(box1[...,2], box2[...,2])\n",
        "    U_y_max = torch.max(box1[...,3], box2[...,3])\n",
        "    U_area = (U_x_max - U_x_min) * (U_y_max - U_y_min)\n",
        "    I_x_min = torch.max(box1[...,0], box2[...,0])\n",
        "    I_y_min = torch.max(box1[...,1], box2[...,1])\n",
        "    I_x_max = torch.min(box1[...,2], box2[...,2])\n",
        "    I_y_max = torch.min(box1[...,3], box2[...,3])\n",
        "    I_area = (I_x_max - I_x_min) * (I_y_max - I_y_min)\n",
        "    I_area[I_x_min >= I_x_max] = 0\n",
        "    I_area[I_y_min >= I_y_max] = 0\n",
        "    return I_area / U_area\n",
        "\n",
        "class Pascal_Dataset(object):\n",
        "    def __init__(self, json_file, dir_img, transform=None, k=9):\n",
        "        with open(json_file, 'r') as file:\n",
        "            self.json_file = json.load(file)\n",
        "        self.dir_img = dir_img\n",
        "        self.imgs_name, self.imgs_size = self.get_img_name()\n",
        "        self.bboxes_img= self.get_bboxes()\n",
        "        self.transform = transform\n",
        "        self.k = k\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.json_file['images'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "            input: idx\n",
        "            output: img:tensor, bbox:tensor\n",
        "        '''\n",
        "        img_path = os.path.join(self.dir_img, self.imgs_name[idx])\n",
        "        img = Image.open(img_path)\n",
        "        bboxes = self.bboxes_img[idx]\n",
        "        bboxes = torch.Tensor(bboxes)\n",
        "        if self.transform:\n",
        "            img, bboxes = self.transform(img, bboxes)\n",
        "        bboxes[:, :2] += bboxes[:, 2:]/2\n",
        "        cls_gt, reg_gt = self.build_gt(img, bboxes)\n",
        "        return img, cls_gt, reg_gt\n",
        "\n",
        "    def get_img_name(self):\n",
        "        imgs = self.json_file['images']\n",
        "        img_name = {img['id']:img['file_name'] for img in imgs}\n",
        "        img_size = {img['id']:[img['height'], img['width']] for img in imgs}\n",
        "        return list(img_name.values()), img_size\n",
        "\n",
        "    def get_bboxes(self):\n",
        "        imgs = self.json_file['annotations']\n",
        "        bboxes = {}\n",
        "        for bbox in imgs:\n",
        "            if bbox['image_id'] not in bboxes:\n",
        "                bboxes[bbox['image_id']] = []\n",
        "\n",
        "            bboxes[bbox['image_id']].append(self.resize(bbox['image_id'], bbox['bbox']))\n",
        "        return list(bboxes.values())\n",
        "\n",
        "    def resize(self, img_id, bbox):\n",
        "        size = self.imgs_size[img_id]\n",
        "        return [bbox[1]/size[0]*size_img[0], bbox[0]/size[1]*size_img[1], bbox[3]/size[0]*size_img[0], bbox[2]/size[1]*size_img[1]]\n",
        "\n",
        "    def build_gt(self, img, bboxes, pos_thresh=0.5, neg_thresh=0.3):\n",
        "        '''\n",
        "            input: img:tensor, bboxes:tensor\n",
        "            output: cls_gt:tensor, reg_gt:tensor\n",
        "        '''\n",
        "        feature_size = get_feature_size(img.shape)\n",
        "        cls_gt = torch.zeros(1*self.k, feature_size[0], feature_size[1])\n",
        "        reg_gt = torch.zeros(feature_size[0], feature_size[1], self.k, 4)\n",
        "        bbox_gt = torch.zeros(feature_size[0], feature_size[1], self.k, 4)\n",
        "        frame_anchor = []\n",
        "        for scale in anchor_size:\n",
        "            for ratio in anchor_ratio:\n",
        "                frame_anchor.append([scale*ratio[0], scale*ratio[1]])\n",
        "        for i in range(feature_size[0]):\n",
        "            for j in range(feature_size[1]):\n",
        "                bbox_gt[i, j] = torch.torch.Tensor([[convert_coord(i), convert_coord(j),frame_anchor[k][0], frame_anchor[k][1]] for k in range(self.k)])\n",
        "\n",
        "        for bbox in bboxes:\n",
        "            IoU_value = IoU(bbox_gt, bbox)\n",
        "            index_pos = torch.argwhere(IoU_value > pos_thresh)\n",
        "            cls_gt[index_pos[:, 2], index_pos[:, 0], index_pos[:, 1]] = 1\n",
        "            reg_gt[index_pos[:, 0], index_pos[:, 1], index_pos[:, 2]] = self.build_offset(bbox_gt[index_pos[:, 0], index_pos[:, 1], index_pos[:, 2]], bbox)\n",
        "        return cls_gt, (torch.flatten(reg_gt, start_dim=2)).permute(2, 0, 1)\n",
        "\n",
        "    def build_offset(self, anchor, bbox):\n",
        "        '''\n",
        "            input: anchor:tensor, bbox:tensor\n",
        "            output: offset:tensor\n",
        "        '''\n",
        "        t = torch.zeros(anchor.shape)\n",
        "        t[...,0] = (bbox[0] - anchor[...,0])/anchor[...,2]\n",
        "        t[...,1] = (bbox[1] - anchor[...,1])/anchor[...,3]\n",
        "        t[...,2] = torch.log(bbox[2]/anchor[...,2])\n",
        "        t[...,3] = torch.log(bbox[3]/anchor[...,3])\n",
        "        return t\n",
        "\n"
      ],
      "metadata": {
        "id": "Wwk_FGZdFp4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Compose():\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, img, bboxes):\n",
        "        for t in self.transforms:\n",
        "            img, bboxes = t(img), bboxes\n",
        "        return img, bboxes"
      ],
      "metadata": {
        "id": "Ana_LutP1izc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 125 # used seed: 123, 124\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
        "BATCH_SIZE = 32\n",
        "WEIGHT_DECAY = 0.0005\n",
        "EPOCHS = 30\n",
        "NUM_WORKERS = 2\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = True\n",
        "LOAD_MODEL_FILE = \"f-RPN_20_epochs.pth\"\n",
        "IMG_DIR = '/content/pascal-voc-2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages'"
      ],
      "metadata": {
        "id": "l8Qe1h-A_CrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = Compose([transforms.Resize(size_img), transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "WwlYMhnZG6Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_back_bone = CNN_back_bone.to(DEVICE)\n"
      ],
      "metadata": {
        "id": "_FGqycNgw_EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_fn(train_loader, model, optimizer, loss_fn):\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "    mean_loss = []\n",
        "    CNN_back_bone.eval()\n",
        "    for batch_idx, (x, cls_gt, reg_gt) in enumerate(loop):\n",
        "        x, cls_gt, reg_gt = x.to(DEVICE), cls_gt.to(DEVICE), reg_gt.to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            feature_map = CNN_back_bone(x)\n",
        "        cls_pred, reg_pred = model(feature_map)\n",
        "        batch_cls_pred = torch.zeros((cls_gt.shape[0], 256, 1)).to(DEVICE)\n",
        "        batch_cls_gt = torch.zeros((cls_gt.shape[0], 256, 1)).to(DEVICE)\n",
        "        for i in range(cls_pred.shape[0]):\n",
        "            mini_cls_pred = cls_pred[i].reshape(-1, 1)\n",
        "            mini_cls_gt = cls_gt[i].reshape(-1, 1)\n",
        "            pos_gt = torch.where(mini_cls_gt == 1)[0]\n",
        "            neg_gt = torch.where(mini_cls_gt == 0)[0]\n",
        "            nums = min(128, pos_gt.size()[0])\n",
        "            pos_gt_size = pos_gt.size()[0]\n",
        "            if pos_gt_size > 0:\n",
        "                index = torch.cat((pos_gt[torch.randint(0, pos_gt.size()[0], (nums,))],\n",
        "                                neg_gt[torch.randint(0, neg_gt.size()[0], (256-nums, ))]))\n",
        "                batch_cls_pred[i] = mini_cls_pred[index]\n",
        "                batch_cls_gt[i] = mini_cls_gt[index]\n",
        "\n",
        "        loss = loss_fn(batch_cls_pred, batch_cls_gt, reg_pred, reg_gt)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        mean_loss.append(loss.item())\n",
        "        optimizer.step()\n",
        "\n",
        "        #Update the progress bar\n",
        "        loop.set_postfix(loss = loss.item())\n",
        "    print(f'Mean loss was {sum(mean_loss)/len(mean_loss)}')\n"
      ],
      "metadata": {
        "id": "f8h7DoYRBh-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(checkpoint_path, model, optimizer):\n",
        "    # Load the checkpoint from the file\n",
        "    # checkpoint = torch.load(checkpoint_path) # This line is incorrect. We've already loaded it\n",
        "\n",
        "    # checkpoint_path is already the loaded checkpoint dictionary. Rename for clarity\n",
        "    checkpoint = checkpoint_path\n",
        "\n",
        "    # Load the model state_dict and optimizer state_dict from the checkpoint\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ],
      "metadata": {
        "id": "R6f4aIXbX_1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RPN(512).to(DEVICE)\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "loss_fn = RPNLoss()\n",
        "\n",
        "if LOAD_MODEL:\n",
        "    load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n",
        "\n",
        "# if LOAD_MODEL:\n",
        "#     load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n",
        "\n",
        "FILE_TRAIN = '/content/pascal-voc-2007/pascal_voc/PASCAL_VOC/pascal_train2007.json'  # Thay thế bằng tên tệp JSON của bạn\n",
        "FILE_TEST = '/content/pascal-voc-2007/pascal_voc/PASCAL_VOC/pascal_test2007.json'  # Thay thế bằng tên tệp JSON của bạn\n",
        "DIR_IMG_TEST = '/content/pascal-voc-2007/VOCtest_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages'\n",
        "\n",
        "train_dataset = Pascal_Dataset(\n",
        "    FILE_TRAIN,\n",
        "    IMG_DIR,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = Pascal_Dataset(\n",
        "    FILE_TEST,\n",
        "    DIR_IMG_TEST,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY,\n",
        "    shuffle=True,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        ")"
      ],
      "metadata": {
        "id": "TW-yeHR7CVoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    # pred_boxes, target_boxes = get_bboxes(\n",
        "    #     train_loader, model, iou_threshold=0.5, threshold=0.4\n",
        "    # )\n",
        "    # mean_avg_prec = mean_average_precision(\n",
        "    #     pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n",
        "    # )\n",
        "    # print(f\"Train mAP: {mean_avg_prec}\")\n",
        "    print(f'epoch: {epoch}')\n",
        "    train_fn(train_loader, model, optimizer, loss_fn)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KXXTrV4PEwP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({'model_state_dict':model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(),}, 'd-RPN-20_epochs.pth')"
      ],
      "metadata": {
        "id": "7-dj3LSrt9o8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, cls_gt, reg_gt = train_dataset[1]\n",
        "x = x.unsqueeze(0).to(DEVICE)\n",
        "cls_gt = cls_gt.unsqueeze(0).to(DEVICE)\n",
        "reg_gt = reg_gt.unsqueeze(0).to(DEVICE)\n",
        "t = CNN_back_bone(x)\n",
        "cls_pred, reg_pred = model(t)"
      ],
      "metadata": {
        "id": "Ty-inAxIAOdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.argwhere(cls_pred[cls_gt > 0] > 0.6).shape[0] / torch.argwhere(cls_pred > 0.6).shape[0]\n"
      ],
      "metadata": {
        "id": "YWTlgfJsXJab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evalue(test_loader, model):\n",
        "    loop = tqdm(test_loader, leave=True)\n",
        "    CNN_back_bone.eval()\n",
        "    for test_idx, (x, cls_gt, reg_gt) in enumerate(loop):\n",
        "        with torch.no_grad():\n",
        "            feature_map =  CNN_back_bone(x)\n"
      ],
      "metadata": {
        "id": "FZsrvRZCUZR9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}