# -*- coding: utf-8 -*-
"""new_RPN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10w1d7s5davDoWOtR33yKLt6uAQMAK9TI

# **Import các thư viện cần thiết**
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import cv2
import os
import json
import numpy as np
from PIL import Image
import torch.optim as optim
import matplotlib.pyplot as plt
import pandas as pd
from tqdm import tqdm
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from collections import Counter

"""# **Tải mô hình VGG16 đã được huấn luyện trước**:
=> Để trích xuất feature maps làm đầu vào cho RPN. Ở đây chỉ lấy phần đặc trưng (features), bỏ qua các lớp fully connected.

## Một số thông tin về trích xuất đặc trưng bằng VGG 16

- VGG16 có kiến trúc bao gồm nhiều lớp tích chập và pooling giúp nó tạo ra các bản đồ đặc trưng (feature maps) từ ảnh. Các lớp này có khả năng trích xuất đặc trưng theo nhiều cấp độ khác nhau, từ các đặc trưng đơn giản (ví dụ như cạnh viền) đến các đặc trưng phức tạp (ví dụ như khuôn mặt, đồ vật).

"""

VGG16_model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)

"""# **Xây dựng mạng Backbone**

Backbone sẽ nhận đầu vào là ảnh và tạo ra feature map (bản đồ đặc trưng) làm đầu vào cho các lớp tiếp theo.
Ở đây sẽ chỉ lấy các feature và bỏ qua fully connected layers (dùng cho classification)


"""

CNN_back_bone = nn.Sequential(*list(VGG16_model.features.children())[:-1])
# list(VGG16_model.features.children(): chuyển các lớp con thành một danh sách
# [:-1]: bỏ lớp cuối (lớp pooling)

anchor_ratio = [[1, 1], [1, 2], [2, 1]] #tỉ lệ các anchor
anchor_size = [8, 16, 32]

class RPN(nn.Module):
    def __init__(self,input_channels, k=9):
        #Convolution Layers
        super(RPN,self).__init__()
        self.conv = nn.Conv2d(input_channels, 512, 3, 1, 1) # 512 filters, kernel 3X3
        self.cls = nn.Conv2d(512, k, 1) #=0 -> k chứa vật thể / =1 -> chứa vật thể
        self.reg = nn.Conv2d(512, 4 * k, 1) #


    def forward(self, x):
        '''
            output: cls:[B, 9, H, W], reg:[B, 36, H, W]
        '''
        x = F.relu(self.conv(x))
        cls = F.sigmoid(self.cls(x))
        reg = self.reg(x)
        return cls, reg

class RPNLoss(nn.Module):
    def __init__(self):
        super(RPNLoss,self).__init__()
        self.cls_loss = nn.BCELoss()
        self.reg_loss = nn.SmoothL1Loss(reduction='mean')
        self.lam_reg = 10
        self.k = 9
    #Tính toán tổng hợp hợp loss cho RPN
    def forward(self, cls_pred, cls_gt, reg_pred, reg_gt, inside_weight):
        '''
            cls_shape = [B, T, 1] T = 256 : Batch size, T số lượng anchor, số lượng phân loại nhãn cho mỗi anchor -> 1
            reg_shape = [B, N, M, 4] N*M = 2400 : Batch size, N chiều cao, M chiều rộng của feature maps, 4 giá trị cần dự đoán
        '''
        nums = cls_gt.shape[1] ###
        # cls loss
        cls_loss = self.cls_loss(cls_pred, cls_gt)


        # reg loss
        shape = reg_gt.shape
        reg_gt = reg_gt.permute(0, 2, 3, 1).view(shape[0], shape[2], shape[3], -1, 4)
        reg_pred = reg_pred.permute(0, 2, 3, 1).view(shape[0], shape[2], shape[3], -1, 4)
        inside_weight = inside_weight.permute(0, 2, 3, 1).unsqueeze(-1)
        selected_reg_pred = reg_pred * inside_weight
        selected_reg_gt = reg_gt * inside_weight
        reg_loss = self.reg_loss(selected_reg_pred, selected_reg_gt)
        for i in range(reg_loss.dim()-1, 0, -1):
            reg_loss = reg_loss.sum(i)
        reg_loss = reg_loss.mean() / nums

        loss = cls_loss + self.lam_reg * reg_loss
        return loss

loss_fn = RPNLoss()

!kaggle datasets download -d zaraks/pascal-voc-2007

!unzip pascal-voc-2007.zip -d pascal-voc-2007

size_img = [600, 600]

def convert_coord(coord, scale=16):
    return coord*scale

def get_feature_size(size, scale=16):
    return [size[-2]//scale, size[-1]//scale]

def convert_box(bbox, btype='edge'):
    bboxes = bbox.clone()
    if (btype == 'edge'): # center -> edge
        bboxes[...,:2] -= bboxes[..., 2:]/2 - 0.5
        bboxes[...,2:] += bboxes[..., :2]-1
        return bboxes

    elif (btype == 'center'): # edge -> center
        bboxes[..., :2] += bboxes[..., 2:]
        bboxes[..., :2] /= 2
        bboxes[..., 2:] -= bboxes[..., :2] - 0.5
        bboxes[..., 2:] *= 2
        return bboxes

def IoU(box1, box2):
    box1 = convert_box(box1).unsqueeze(1)
    box2 = convert_box(box2).unsqueeze(0)
    Area1 = (box1[...,2] - box1[...,0]+1) * (box1[...,3] - box1[...,1]+1)
    Area2 = (box2[...,2] - box2[...,0]+1) * (box2[...,3] - box2[...,1]+1)
    I_x_min = torch.max(box1[...,0], box2[...,0])
    I_y_min = torch.max(box1[...,1], box2[...,1])
    I_x_max = torch.min(box1[...,2], box2[...,2])
    I_y_max = torch.min(box1[...,3], box2[...,3])
    I_area = (I_x_max - I_x_min+1) * (I_y_max - I_y_min+1)
    U_area = Area1 + Area2 - I_area
    I_area[I_x_min >= I_x_max] = 0
    I_area[I_y_min >= I_y_max] = 0
    return I_area / U_area

class Pascal_Dataset(object):
    def __init__(self, json_file, dir_img, training_set=True, transform=None, k=9, img_sized = (3, 600, 600)):
        with open(json_file, 'r') as file:
            self.json_file = json.load(file)
        self.dir_img = dir_img
        self.imgs_name, self.imgs_size = self.get_img_name()
        self.img_sized = img_sized
        self.bboxes_img= self.get_bboxes()
        self.transform = transform
        self.k = k
        self.fg_nums = 128
        self.total_nums = 256
        self.anchor_size = 16
        self.training_set = training_set
        self.all_anchor = self.create_all_anchor()

    def __len__(self):
        return len(self.json_file['images'])

    def __getitem__(self, idx):
        '''
            input: idx
            output: img:tensor, bbox:tensor
        '''
        img_path = os.path.join(self.dir_img, self.imgs_name[idx])
        img = Image.open(img_path)
        bboxes = self.bboxes_img[idx]
        bboxes = torch.Tensor(bboxes)
        if self.transform:
            img, bboxes = self.transform(img, bboxes)
        bboxes[:, :2] += bboxes[:, 2:]/2
        cls_gt, reg_gt, inside_weight = self.build_gt(bboxes)
        if self.training_set:
            return img, cls_gt, reg_gt, inside_weight
        else:
            bboxes_nums = bboxes.shape[0]
            bboxes = torch.cat((bboxes, torch.zeros(50-bboxes_nums, 4)))
            return img, cls_gt, reg_gt, bboxes

    def get_img_name(self):
        imgs = self.json_file['images']
        img_name = {img['id']:img['file_name'] for img in imgs}
        img_size = {img['id']:[img['height'], img['width']] for img in imgs}
        return list(img_name.values()), img_size

    def get_bboxes(self):
        imgs = self.json_file['annotations']
        bboxes = {}
        for bbox in imgs:
            if bbox['image_id'] not in bboxes:
                bboxes[bbox['image_id']] = []

            bboxes[bbox['image_id']].append(self.resize(bbox['image_id'], bbox['bbox']))
        return list(bboxes.values())

    def resize(self, img_id, bbox):
        size = self.imgs_size[img_id]
        return [bbox[1]/size[0]*size_img[0], bbox[0]/size[1]*size_img[1], bbox[3]/size[0]*size_img[0], bbox[2]/size[1]*size_img[1]]

    def build_gt(self, bboxes, pos_thresh=0.7, neg_thresh=0.3):
        '''
            input: img:tensor, bboxes:tensor
            output: cls_gt:tensor, reg_gt:tensor
        '''
        height, width = get_feature_size(self.img_sized)
        all_anchors = self.all_anchor.clone()
        anchors_size = all_anchors.shape[:2]
        keep = ((all_anchors[...,0] - all_anchors[...,2]/2 >= 0) &
                (all_anchors[...,1] - all_anchors[...,3]/2 >= 0) &
                (all_anchors[...,0] + all_anchors[...,2]/2 < self.img_sized[1]) &
                (all_anchors[...,1] + all_anchors[...,3]/2 < self.img_sized[2]))
        ind_inside = torch.nonzero(keep)
        inside_anchors = all_anchors[ind_inside[:, 0], ind_inside[:, 1]]

        label = -torch.ones((ind_inside.shape[0],))
        bbox_inside_weight = torch.zeros((ind_inside.shape[0],))
        bbox_outside_weight = torch.ones((ind_inside.shape[0],))

        overlap = IoU(inside_anchors, bboxes)
        max_overlap, argmax_overlap = torch.max(overlap, dim=1)

        pos_index = torch.argwhere(max_overlap > pos_thresh)
        neg_index = torch.argwhere(max_overlap < neg_thresh)
        if (pos_index.shape[0] > self.fg_nums):
            pos_index = pos_index[torch.randperm(pos_index.shape[0])[:self.fg_nums]]

            label[pos_index] = 1
            bbox_inside_weight[pos_index] = 1
        else:
            label[pos_index] = 1
            bbox_inside_weight[pos_index] = 1
        bg_nums = self.total_nums - min(self.fg_nums, pos_index.shape[0])
        if (neg_index.shape[0] > bg_nums):
            neg_index = neg_index[torch.randperm(neg_index.shape[0])[:bg_nums]]
            label[neg_index] = 0
        bbox_inside_weight[label == 1] = 1

        bbox_outside_weight[label == 1] = 1/self.total_nums
        bbox_outside_weight[label == 0] = 1/self.total_nums

        bbox_offset = self.build_offset(inside_anchors, bboxes[argmax_overlap])

        label = self.unmap(label, anchors_size, ind_inside, -1)
        bbox_offset = self.unmap(bbox_offset, anchors_size, ind_inside)
        bbox_inside_weight = self.unmap(bbox_inside_weight, anchors_size, ind_inside)
        bbox_outside_weight = self.unmap(bbox_outside_weight, anchors_size, ind_inside)


        # all_anchors = all_anchors.view(width, height, self.k, 4).permute(1, 0, 2, 3) # ma trận có height thay đổi trước, nên ta cần hiệu chỉnh lại
        # all_anchors = all_anchors.view(height, width, -1)
        # all_anchors = all_anchors.permute(1, 0, 2).view(-1, 4)
        # (w, h, k, 4) -> (h, w, k, 4)
        cls_gt = label.view(width, height, self.k).permute(2, 1, 0)
        reg_gt = bbox_offset.view(width, height, self.k, 4).permute(1, 0, 2, 3).view(height, width, 4 * self.k).permute(2, 0, 1)
        bbox_inside_weight = bbox_inside_weight.view(width, height, self.k).permute(2, 1, 0)
        bbox_outside_weight = bbox_outside_weight.view(width, height, self.k).permute(2, 1, 0)

        return cls_gt, reg_gt, bbox_inside_weight

    def create_all_anchor(self, img_size = (3, 600, 600)):
        anchors = self.generate_anchor()


        feat_scale = 16
        feat_size = get_feature_size(img_size)
        y_grid = torch.arange(feat_size[0]) * feat_scale
        x_grid = torch.arange(feat_size[1]) * feat_scale
        y_grid, x_grid = torch.meshgrid(y_grid, x_grid)
        y_grid = torch.flatten(y_grid)
        x_grid = torch.flatten(x_grid)

        feat_map = torch.stack((x_grid, y_grid, torch.zeros_like(x_grid), torch.zeros_like(y_grid)), dim=1).unsqueeze(1)

        all_anchor = feat_map + anchors
        return all_anchor

    def generate_anchor(self, anchor_base = 16):
        '''
            input: ratios:list, scale:list
            output: anchor_box:tensor
        '''
        ratios = torch.Tensor(anchor_ratio)
        scale = torch.Tensor(anchor_size)
        anchor_box = torch.tensor([1, 1, anchor_base, anchor_base], dtype=torch.float) - 1
        anchor_box = convert_box(anchor_box, 'center')
        anchor_side = anchor_box[2:].unsqueeze(0)
        anchor_side = anchor_box[2:].unsqueeze(0) * (ratios.unsqueeze(1) * scale.unsqueeze(1)).view(-1, 2)
        anchor_box = torch.cat((anchor_box[:2].unsqueeze(0).expand(len(anchor_ratio) * len(anchor_size), 2), anchor_side) , dim=-1)
        anchor_box = anchor_box.view(1, -1, 4)
        return anchor_box

    def unmap(self, data, count, inds, fill=0):
        if (data.dim() == 1):
            res = torch.zeros(count[0], count[1]) + fill
            res[inds[:, 0], inds[:, 1]] = data
            return res
        else:
            res = torch.zeros(count[0], count[1], data.shape[-1]) + fill
            res[inds[:, 0], inds[:, 1]] = data
            return res

    def build_offset(self, anchor, bbox):
        '''
            input: anchor:tensor, bbox:tensor
            output: offset:tensor
        '''
        t = torch.zeros(anchor.shape)
        t[...,0] = (bbox[...,0] - anchor[...,0])/anchor[...,2]
        t[...,1] = (bbox[...,1] - anchor[...,1])/anchor[...,3]
        t[...,2] = torch.log(bbox[...,2]/anchor[...,2])
        t[...,3] = torch.log(bbox[...,3]/anchor[...,3])
        return t

class Compose():
    def __init__(self, transforms):
        self.transforms = transforms

    def __call__(self, img, bboxes):
        for t in self.transforms:
            img, bboxes = t(img), bboxes
        return img, bboxes

seed = 125 # used seed: 123, 124
torch.manual_seed(seed)

LEARNING_RATE = 1e-3
DEVICE = "cuda" if torch.cuda.is_available else "cpu"
# DEVICE = 'cpu'
BATCH_SIZE = 32
WEIGHT_DECAY = 0
EPOCHS = 3
NUM_WORKERS = 2
PIN_MEMORY = True
LOAD_MODEL = False
LOAD_MODEL_FILE = "f-RPN_20_epochs.pth"
IMG_DIR = '/content/pascal-voc-2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages'

transform = Compose([transforms.Resize(size_img), transforms.ToTensor()])

CNN_back_bone = CNN_back_bone.to(DEVICE)

def train_fn(train_loader, model, optimizer, loss_fn):
    loop = tqdm(train_loader, leave=True)
    mean_loss = []
    CNN_back_bone.eval()
    for batch_idx, (x, cls_gt, reg_gt, inside_weight) in enumerate(loop):
        x, cls_gt, reg_gt, inside_weight = x.to(DEVICE), cls_gt.to(DEVICE), reg_gt.to(DEVICE), inside_weight.to(DEVICE)
        with torch.no_grad():
            feature_map = CNN_back_bone(x)
        cls_pred, reg_pred = model(feature_map)
        batch_cls_pred = cls_pred[cls_gt >= 0].view(cls_pred.size(0), -1)
        batch_cls_gt = cls_gt[cls_gt >= 0].view(cls_pred.size(0), -1)
        loss = loss_fn(batch_cls_pred, batch_cls_gt, reg_pred, reg_gt, inside_weight)
        optimizer.zero_grad()
        loss.backward()
        mean_loss.append(loss.item())
        optimizer.step()

        #Update the progress bar
        loop.set_postfix(loss = loss.item())
    print(f'Mean loss was {sum(mean_loss)/len(mean_loss)}')

def load_checkpoint(checkpoint_path, model, optimizer):
    # Load the checkpoint from the file
    # checkpoint = torch.load(checkpoint_path) # This line is incorrect. We've already loaded it

    # checkpoint_path is already the loaded checkpoint dictionary. Rename for clarity
    checkpoint = checkpoint_path

    # Load the model state_dict and optimizer state_dict from the checkpoint
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

model = RPN(512).to(DEVICE)

optimizer = optim.Adam(
    model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY
)
loss_fn = RPNLoss()

if LOAD_MODEL:
    load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)

# if LOAD_MODEL:
#     load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)

FILE_TRAIN = '/content/pascal-voc-2007/pascal_voc/PASCAL_VOC/pascal_train2007.json'  # Thay thế bằng tên tệp JSON của bạn
FILE_TEST = '/content/pascal-voc-2007/pascal_voc/PASCAL_VOC/pascal_test2007.json'  # Thay thế bằng tên tệp JSON của bạn
DIR_IMG_TEST = '/content/pascal-voc-2007/VOCtest_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages'

train_dataset = Pascal_Dataset(
    FILE_TRAIN,
    IMG_DIR,
    transform=transform
)
print(f'Số lượng ảnh trong tập huấn luyện: {len(train_dataset)}')

test_dataset = Pascal_Dataset(
    FILE_TEST,
    DIR_IMG_TEST,
    transform=transform
)

train_loader = DataLoader(
    dataset=train_dataset,
    batch_size=BATCH_SIZE,
    num_workers=NUM_WORKERS,
    pin_memory=PIN_MEMORY,
    shuffle=True,
    drop_last=False,
)

test_loader = DataLoader(
    dataset=test_dataset,
    batch_size=BATCH_SIZE,
    num_workers=NUM_WORKERS,
    pin_memory=PIN_MEMORY,
    shuffle=True,
    drop_last=True,
)

LOAD_MODEL_FILE = "d07-RPN-20_epochs.pth"

if LOAD_MODEL:
    load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)

NEW_LEARNING_RATE = 5e-4
NEW_WEIGHT_DECAY = 1e-5

for param_group in optimizer.param_groups:
    param_group['lr'] = NEW_LEARNING_RATE  # Thay đổi lr
    param_group['weight_decay'] = NEW_WEIGHT_DECAY
seed = 135 # used seed: 123, 124
torch.manual_seed(seed)

for epoch in range(EPOCHS):
    print(f'epoch: {epoch + 1} / {EPOCHS}')
    train_fn(train_loader, model, optimizer, loss_fn)

torch.save({'model_state_dict':model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(),}, 'd07-RPN-20_epochs.pth')

k = 9

anchor_box = []

for ratio in anchor_ratio:
    for scale in anchor_size:
        anchor_box.append([scale*ratio[0], scale*ratio[1]])

anchor_box = torch.Tensor(anchor_box).view(-1).to(DEVICE)
#anchor_box = torch.Tensor(anchor_box).to(DEVICE)  # Hình dạng sẽ là [num_anchors, 2]

def rebuild_bbox(reg):
    """
    Rebuilds bounding boxes from the regression output.

    Args:
        reg: Regression output tensor. Expected to be [batch_size, num_channels, height, width].

    Returns:
        bbox: Rebuilt bounding boxes tensor with shape [batch_size, num_anchors, 4].
    """
    # Reshape reg to have shape [batch_size, num_anchors, 4]
    reg = reg.permute(0, 2, 3, 1).reshape(reg.shape[0], -1, 4)

    # Clone to preserve original tensor structure
    bbox = reg.clone()
    # Assuming you have the anchor_box available
    anchors_x = anchor_box[:, 0]  # Tọa độ x của các anchor
    anchors_y = anchor_box[:, 1]  # Tọa độ y của các anchor

    bbox[..., 0::4] = reg[..., 0] + anchors_x  # điều chỉnh x1
    bbox[..., 1::4] = reg[..., 1] + anchors_y  # điều chỉnh y1
    bbox[..., 2::4] = torch.exp(reg[..., 2]) * anchor_box[..., 0]  # điều chỉnh x2
    bbox[..., 3::4] = torch.exp(reg[..., 3]) * anchor_box[..., 1]  # điều chỉnh y2



    return bbox



def calculate_iou(bbox_pred, bbox_gt):
    """
    Calculate Intersection over Union (IoU) between predicted and ground truth bounding boxes.

    Args:
        bbox_pred: Predicted bounding boxes with shape [num_preds, 4].
        bbox_gt: Ground truth bounding boxes with shape [num_gt, 4].

    Returns:
        iou: IoU tensor of shape [num_preds, num_gt].
    """
    # Check if the input bounding boxes are valid
    if bbox_pred.size(1) != 4 or bbox_gt.size(1) != 4:
        raise ValueError("Bounding boxes must have shape [num_boxes, 4]")

    # Expand dimensions to compare each predicted box with every ground truth box
    bbox_pred = bbox_pred.unsqueeze(1)  # Shape: [num_preds, 1, 4]
    bbox_gt = bbox_gt.unsqueeze(0)  # Shape: [1, num_gt, 4]

    # Calculate intersection coordinates
    x1_inter = torch.max(bbox_pred[..., 0], bbox_gt[..., 0])
    y1_inter = torch.max(bbox_pred[..., 1], bbox_gt[..., 1])
    x2_inter = torch.min(bbox_pred[..., 2], bbox_gt[..., 2])
    y2_inter = torch.min(bbox_pred[..., 3], bbox_gt[..., 3])

    # Calculate intersection area
    inter_area = torch.clamp(x2_inter - x1_inter, min=0) * torch.clamp(y2_inter - y1_inter, min=0)

    # Calculate areas of the predicted and ground truth boxes
    bbox_pred_area = (bbox_pred[..., 2] - bbox_pred[..., 0]) * (bbox_pred[..., 3] - bbox_pred[..., 1])
    bbox_gt_area = (bbox_gt[..., 2] - bbox_gt[..., 0]) * (bbox_gt[..., 3] - bbox_gt[..., 1])

    # Calculate union area
    union_area = bbox_pred_area + bbox_gt_area - inter_area

    # Avoid division by zero
    iou = inter_area / torch.clamp(union_area, min=1e-6)

    return iou

def cal_cls_accuracy(cls_pred, cls_gt, thresh=0.5):
    # Lọc các giá trị gt hợp lệ
    valid_indices = cls_gt != -1
    cls_gt_filtered = cls_gt[valid_indices]
    cls_pred_filtered = cls_pred[valid_indices]

    if cls_gt_filtered.numel() == 0:  # Không có ground truth hợp lệ
        return 0

    true_positive = torch.logical_and(cls_pred_filtered > thresh, cls_gt_filtered == 1).view(-1)
    true_negative = torch.logical_and(cls_pred_filtered <= thresh, cls_gt_filtered == 0).view(-1)

    correct_predictions = torch.nonzero(true_positive).size(0) + torch.nonzero(true_negative).size(0)
    total_predictions = cls_gt_filtered.numel()  # Tổng số dự đoán hợp lệ

    accuracy = correct_predictions / total_predictions
    return accuracy


def cal_reg_accuracy(cls_pred, bbox_pred, bbox_gt, iou_threshold=0.5):

    # Calculate IoU between predicted and ground truth boxes
    iou = calculate_iou(bbox_pred, bbox_gt)

    # Only consider bounding boxes where the model predicted an object (cls_pred > 0.5)
    predicted_objects = cls_pred > 0.5

    # Count how many predicted boxes have IoU greater than the threshold
    correct_boxes = torch.nonzero(iou > iou_threshold).size(0)
    total_boxes = predicted_objects.sum().item()

    if total_boxes == 0:
        return 0  # Handle cases where no objects are predicted

    accuracy = correct_boxes / total_boxes
    return accuracy

def evaluate(test_loader, model):

    model.eval()  # Đặt mô hình vào chế độ đánh giá
    cls_accuracies = []  # Danh sách lưu trữ độ chính xác phân loại
    reg_accuracies = []  # Danh sách lưu trữ độ chính xác hồi quy

    with torch.no_grad():  # Tắt tính toán gradient
        for x, cls_gt, reg_gt, bboxes in test_loader:
            x, cls_gt, reg_gt, bboxes = x.to(DEVICE), cls_gt.to(DEVICE), reg_gt.to(DEVICE), bboxes.to(DEVICE)

            # Truyền qua mô hình
            feature_map = CNN_back_bone(x)
            cls_pred, reg_pred = model(feature_map)

            # Lọc các ground truth hợp lệ
            valid_indices = cls_gt != -1
            cls_gt_filtered = cls_gt[valid_indices]
            cls_pred_filtered = cls_pred[valid_indices]

            # Tính toán độ chính xác phân loại
            if cls_gt_filtered.numel() > 0:  # Kiểm tra nếu có ground truth hợp lệ
                cls_accuracy = cal_cls_accuracy(cls_pred_filtered, cls_gt_filtered)
                cls_accuracies.append(cls_accuracy)

            # Tính toán độ chính xác hồi quy
            bbox_pred = rebuild_bbox(reg_pred)  # Xây dựng lại các bounding boxes dự đoán
            reg_accuracy = cal_reg_accuracy(cls_pred, bbox_pred, reg_gt)  # Tính toán độ chính xác hồi quy
            reg_accuracies.append(reg_accuracy)

    # Tính toán và in ra độ chính xác trung bình
    mean_cls_accuracy = sum(cls_accuracies) / len(cls_accuracies) if cls_accuracies else 0
    mean_reg_accuracy = sum(reg_accuracies) / len(reg_accuracies) if reg_accuracies else 0

    print(f'Mean Classification Accuracy: {mean_cls_accuracy:.4f}')
    print(f'Mean Regression Accuracy (IoU): {mean_reg_accuracy:.4f}')





#evaluate(train_loader, model)